---
title       : Neural Networks
subtitle    : A really brief firehosing
author      : Tommy Shen
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : prettify  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---&twocol w1:60% w2:40%
## A more challenging problem
*** left

```{r, echo=FALSE, fig.height=7, fig.width=7}
plot(range(-0.2, 1.2), range(-0.2, 1.2), type='n', xlab="dimension a", ylab="dimension b")
points(c(0,1), c(0,1), col='dark green', pch=16, cex=3)
points(c(1,0), c(0,1), col='dark red', pch=16, cex=3)
```

*** right

- How would we classify this problem with the methodologies we've learned thus far?
  * KNN
  * Naive Bayes
  * Regression
  * Trees

--- 
## Gateway to Complex Modeling
Neural nets (or neural networks) are computational models patterened after the biological processes of the brain, specifically neurons.

Neural networks allow for the modeling of complex patterns due to its ability to create deep architectures. (*Spoiler: More to come in deep learning talk.*)

Neural networks come in many learning paradigms:
- supervised 
- unsupervised
- reinforcement

---

## This is a neuron
![Image](./images/neuron.gif)
  
<div class='source'>
  Source: <a href='webspace.ship.edu'>Picture from webspace.ship.edu</a>
</div>

---
## Modeling A Neuron
```{r, echo=FALSE, fig.height=8, fig.width=13}
par(mar=c(2,2,0.5,1))
plot(range(0, 20), range(0, 14), type='n', axes=FALSE, frame.plot=TRUE, xlab='', ylab='')
symbols(10, 7, circles=3.1, bg="white", fg="dark red", add=TRUE, lwd=4)

```
---
## Add some dendrites
```{r, echo=FALSE, fig.height=8, fig.width=13}
par(mar=c(2,2,0.5,1))
plot(range(0, 20), range(0, 14), type='n', axes=FALSE, frame.plot=TRUE, xlab='', ylab='')
symbols(10, 7, circles=3.1, bg="white", fg="dark red", add=TRUE, lwd=4)
arrows(c(2,2,2), c(2,7,12), c(7.8, 7.8, 7.8), c(5.0, 7, 9.0), lwd=3)

```

---
## And the axon/axon terminal

```{r, echo=FALSE, fig.height=8, fig.width=13}
par(mar=c(2,2,0.5,1))
plot(range(0, 20), range(0, 14), type='n', axes=FALSE, frame.plot=TRUE, xlab='', ylab='')
symbols(10, 7, circles=3.1, bg="white", fg="dark red", add=TRUE, lwd=4)
arrows(c(2,2,2,13), c(2,7,12,7), c(7.8, 7.8, 7.8, 18), c(5.0, 7, 9.0, 7), lwd=3)

```

---
## Add some math!

```{r, echo=FALSE, fig.height=8, fig.width=13}
par(mar=c(2,2,0.5,1))
plot(range(0, 20), range(0, 14), type='n', axes=FALSE, frame.plot=TRUE, xlab='', ylab='')
symbols(10, 7, circles=3.1, bg="white", fg="dark red", add=TRUE, lwd=4)
arrows(c(2,2,2,13, 10), c(2,7,12,7, 10.8), c(7.8, 7.8, 7.8, 18, 10), c(5.0, 7, 9.0, 7, 9.4), lwd=3)
text(c(1.6,1.6,1.6), c(2,7,12), labels=c(expression(x[3]), expression(x[2]), expression(x[1])), cex=2.5)
text(c(5,5,5, 15.2, 10),c(4,7.5,11, 7.5, 11.5), labels=c(expression(w[3]), expression(w[2]), expression(w[1]), "output (y)", expression(paste("Bias ", theta))), cex=2.5)
text(10,7, labels=expression(f(w[i]*x[i])), cex=2.8)
text(13, 3, labels=expression(paste('y = ', f(w[i]*x[i]) + theta)), cex=2.5)
```
---
## Nodes forming a network

![Image](./images/neural_network_1.png)
  
<div class='source'>
  Source: <a href='www.astroml.org'>Picture from www.astroml.org</a>
</div>

---
## Neuron Activation Functions
<style>
.MathJax_Display {
  margin: 0em !important; 
}
</style>
1. Linear Neurons: <font size='34px' color=#00000>$$y=b+\sum_{i} x_{i} w_{i}$$</font>

2. Binary Threshold Neurons: <font size='34px' color=#00000 margin-top='0'>$$
z = \sum x_{i} w_{i}\\
y =
\begin{cases}
    1   & z \geq \theta \\
    0              & \text{otherwise}
\end{cases}$$</font>

3. Rectified Linear Neurons: <font size='34px' color=#00000 margin-top='0'>$$
y =
\begin{cases}
    z   & z \geq \theta \\
    0              & \text{otherwise}
\end{cases}$$</font>

---&twocol w1:50% w2:50%
## More on Binary Threshold Neurons

*** left

Binary threshold is the most common choice for modeling individual neuron nodes.
- A sigmoid distribution is often used to model the activation function.  This is the same function used in logistic regression.
- A binary threshold neuron passes a 0 or 1 value to the next layer depending on whether the activation threshold had been met.


*** right

```{r, echo=FALSE, fig.height=7, fig.width=7}
sigmoid <- function(x) {
  1 / ( 1 + exp(-x) )
}
sigmoid_shift <- function(x) {
  1 / ( 1 + exp(-x-2) )
}
par(mar=c(3,3,3,0))
plot(sigmoid, -4, 4, xlab='z', ylab='', main="Sigmoid Function", cex=3)
curve(sigmoid_shift, -4, 4, add=TRUE, lty=2)
arrows(c(0, -1.2), c(0.65, 0.35), c(-0.8, -2.0), c(0.65,0.35))
abline(h=0.5, col='red', lty=2, cex=3)

```

---
## Back to our XNOR problem
```{r, echo=2:4, message=FALSE, fig.height=7.5, fig.width=11, tidy=FALSE, cache=TRUE}
library(neuralnet)
inputs <- data.frame(A=sample(c(0,1), 100,replace=TRUE),B=sample(c(0,1), 100, replace=TRUE))
inputs$XNOR <- ifelse(inputs$A == inputs$B, 1, 0)
net.xnor <- neuralnet(XNOR~A+B, data=inputs, hidden=2, rep=10)
par(mar=c(4,2,0,1))
plot(net.xnor, rep='best', cex=1.6)

```

---
## HALP!!
I needs more material??

